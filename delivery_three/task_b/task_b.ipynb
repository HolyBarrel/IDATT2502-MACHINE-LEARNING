{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrErhla6NwCH",
        "outputId": "b9eebd1c-a400-455c-c21e-4671608513c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 100136832.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 31341706.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31905607.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1803173.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "accuracy = tensor(0.9748)\n",
            "accuracy = tensor(0.9814)\n",
            "accuracy = tensor(0.9812)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "# COPIED FROM https://gitlab.com/ntnu-tdat3025/cnn/mnist/-/blob/master/nn.py?ref_type=heads\n",
        "\n",
        "# task_a.py is a further dev of nn.py\n",
        "\n",
        "# Load observations from the mnist dataset. The observations are divided into a training set and a test set\n",
        "mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True)\n",
        "x_train = mnist_train.data.reshape(-1, 1, 28, 28).float()  # torch.functional.nn.conv2d argument must include channels (1)\n",
        "y_train = torch.zeros((mnist_train.targets.shape[0], 10))  # Create output tensor\n",
        "y_train[torch.arange(mnist_train.targets.shape[0]), mnist_train.targets] = 1  # Populate output\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True)\n",
        "x_test = mnist_test.data.reshape(-1, 1, 28, 28).float()  # torch.functional.nn.conv2d argument must include channels (1)\n",
        "y_test = torch.zeros((mnist_test.targets.shape[0], 10))  # Create output tensor\n",
        "y_test[torch.arange(mnist_test.targets.shape[0]), mnist_test.targets] = 1  # Populate output\n",
        "\n",
        "# Normalization of inputs\n",
        "mean = x_train.mean()\n",
        "std = x_train.std()\n",
        "x_train = (x_train - mean) / std\n",
        "x_test = (x_test - mean) / std\n",
        "\n",
        "# Divide training data into batches to speed up optimization\n",
        "batches = 600\n",
        "x_train_batches = torch.split(x_train, batches)\n",
        "y_train_batches = torch.split(y_train, batches)\n",
        "\n",
        "\n",
        "class ConvolutionalNeuralNetworkModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvolutionalNeuralNetworkModel, self).__init__()\n",
        "\n",
        "        # Model layers (includes initialized model variables):\n",
        "        self.conv = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dense0 = nn.Linear(64 * 7 * 7, 1024)\n",
        "        self.dense = nn.Linear(1024 * 1 * 1, 10)\n",
        "\n",
        "    def logits(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dense0(x.reshape(-1, 64 * 7 * 7))\n",
        "        return self.dense(x.reshape(-1, 1024 * 1 * 1))\n",
        "\n",
        "    # Predictor\n",
        "    def f(self, x):\n",
        "        return torch.softmax(self.logits(x), dim=1)\n",
        "\n",
        "    # Cross Entropy loss\n",
        "    def loss(self, x, y):\n",
        "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
        "\n",
        "    # Accuracy\n",
        "    def accuracy(self, x, y):\n",
        "        return torch.mean(torch.eq(self.f(x).argmax(1), y.argmax(1)).float())\n",
        "\n",
        "\n",
        "model = ConvolutionalNeuralNetworkModel()\n",
        "\n",
        "# Optimize: adjust W and b to minimize loss using stochastic gradient descent\n",
        "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
        "for epoch in range(3):\n",
        "    for batch in range(len(x_train_batches)):\n",
        "        model.loss(x_train_batches[batch], y_train_batches[batch]).backward()  # Compute loss gradients\n",
        "        optimizer.step()  # Perform optimization by adjusting W and b,\n",
        "        optimizer.zero_grad()  # Clear gradients for next step\n",
        "\n",
        "    print(\"accuracy = %s\" % model.accuracy(x_test, y_test))\n",
        "\n",
        "\n"
      ]
    }
  ]
}