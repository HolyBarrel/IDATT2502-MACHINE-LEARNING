{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPjiqYND4yyw",
        "outputId": "2496dbab-599d-4479-abd2-f82508505249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 19571795.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 373333.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 6220466.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 18534143.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "accuracy = tensor(0.8625)\n",
            "accuracy = tensor(0.8821)\n",
            "accuracy = tensor(0.8946)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
        "                                             download=True, transform=transform)\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "\n",
        "\n",
        "# For the training set\n",
        "fashion_train = torchvision.datasets.FashionMNIST('./data', train=True, download=True)\n",
        "x_train = fashion_train.data.reshape(-1, 1, 28, 28).float()  # torch.functional.nn.conv2d argument must include channels (1)\n",
        "y_train = torch.zeros((fashion_train.targets.shape[0], 10))  # Create output tensor\n",
        "y_train[torch.arange(fashion_train.targets.shape[0]), fashion_train.targets] = 1  # Populate output\n",
        "\n",
        "# For the test set\n",
        "fashion_test = torchvision.datasets.FashionMNIST('./data', train=False, download=True)\n",
        "x_test = fashion_test.data.reshape(-1, 1, 28, 28).float()  # torch.functional.nn.conv2d argument must include channels (1)\n",
        "y_test = torch.zeros((fashion_test.targets.shape[0], 10))  # Create output tensor\n",
        "y_test[torch.arange(fashion_test.targets.shape[0]), fashion_test.targets] = 1  # Populate output\n",
        "\n",
        "\n",
        "\n",
        "# Normalization of inputs\n",
        "mean = x_train.mean()\n",
        "std = x_train.std()\n",
        "x_train = (x_train - mean) / std\n",
        "x_test = (x_test - mean) / std\n",
        "\n",
        "# Divide training data into batches to speed up optimization\n",
        "batches = 600\n",
        "x_train_batches = torch.split(x_train, batches)\n",
        "y_train_batches = torch.split(y_train, batches)\n",
        "\n",
        "\n",
        "class ConvolutionalNeuralNetworkModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvolutionalNeuralNetworkModel, self).__init__()\n",
        "\n",
        "        # Model layers (includes initialized model variables):\n",
        "        self.conv = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
        "        self.ReLU = nn.ReLU(inplace = False)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dense0 = nn.Linear(64 * 7 * 7, 1024)\n",
        "        self.dense = nn.Linear(1024 * 1 * 1, 10)\n",
        "\n",
        "    def logits(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dense0(x.reshape(-1, 64 * 7 * 7))\n",
        "        return self.dense(x.reshape(-1, 1024 * 1 * 1))\n",
        "\n",
        "    # Predictor\n",
        "    def f(self, x):\n",
        "        return torch.softmax(self.logits(x), dim=1)\n",
        "\n",
        "    # Cross Entropy loss\n",
        "    def loss(self, x, y):\n",
        "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
        "\n",
        "    # Accuracy\n",
        "    def accuracy(self, x, y):\n",
        "        return torch.mean(torch.eq(self.f(x).argmax(1), y.argmax(1)).float())\n",
        "\n",
        "\n",
        "model = ConvolutionalNeuralNetworkModel()\n",
        "\n",
        "# Optimize: adjust W and b to minimize loss using stochastic gradient descent\n",
        "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
        "for epoch in range(3):\n",
        "    for batch in range(len(x_train_batches)):\n",
        "        model.loss(x_train_batches[batch], y_train_batches[batch]).backward()  # Compute loss gradients\n",
        "        optimizer.step()  # Perform optimization by adjusting W and b,\n",
        "        optimizer.zero_grad()  # Clear gradients for next step\n",
        "\n",
        "    print(\"accuracy = %s\" % model.accuracy(x_test, y_test))\n",
        "\n"
      ]
    }
  ]
}